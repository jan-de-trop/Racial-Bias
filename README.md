# Racial-Bias

Bias-affected datasets can cause extreme deviation in machine learning results.
After pressure from several news agencies and a public investigation by ProPublica, Northpointe, an American tech-company that works with law establishment across several states in the US to predict future crimes based on past records of criminals, released this dataset with a slice of the factors usually considered in order to assign a score to criminals.
It has been speculated that their software- COMPAS used to determine risk scores of previosly convicted people- is biased against the african-american criminals, who end up with high-risk tags, despite minor criminal record, whereas Caucasians regularly received low scores despite more significant criminal charges. We try to see how the bias affects a machine learning system through this dataset.
Recommended reading - ProPublica's: https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing
